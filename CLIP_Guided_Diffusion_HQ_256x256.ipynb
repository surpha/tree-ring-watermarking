{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YwMUyt9LHG1"
      },
      "source": [
        "# Generates images from text prompts with CLIP guided diffusion.\n",
        "\n",
        "It uses OpenAI's 256x256 unconditional ImageNet diffusion model (https://github.com/openai/guided-diffusion) together with CLIP (https://github.com/openai/CLIP) to connect text prompts with images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ3rNuAWAewx"
      },
      "source": [
        "# Check the GPU status\n",
        "\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_UVMZCIAq_r"
      },
      "source": [
        "# Install dependencies\n",
        "\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/crowsonkb/guided-diffusion\n",
        "!pip install -e ./CLIP\n",
        "!pip install -e ./guided-diffusion\n",
        "!pip install lpips"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zAqFEykBHDL"
      },
      "source": [
        "# Download the diffusion model\n",
        "\n",
        "!curl -OL 'https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion_uncond.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmbrcrhpBPC6"
      },
      "source": [
        "# Imports\n",
        "\n",
        "import gc\n",
        "import io\n",
        "import math\n",
        "import sys\n",
        "\n",
        "from IPython import display\n",
        "import lpips\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "sys.path.append('./CLIP')\n",
        "sys.path.append('./guided-diffusion')\n",
        "\n",
        "import clip\n",
        "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHOj78Yvx8jP"
      },
      "source": [
        "# Define necessary functions\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    if prompt.startswith('http://') or prompt.startswith('https://'):\n",
        "        vals = prompt.rsplit(':', 2)\n",
        "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
        "    else:\n",
        "        vals = prompt.rsplit(':', 1)\n",
        "    vals = vals + ['', '1'][len(vals):]\n",
        "    return vals[0], float(vals[1])\n",
        "\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "        return torch.cat(cutouts)\n",
        "\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "\n",
        "def tv_loss(input):\n",
        "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
        "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
        "\n",
        "\n",
        "def range_loss(input):\n",
        "    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        # If the input is a URL, fetch the content\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()  # Raise an exception for HTTP errors\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)  # Write the content to a BytesIO buffer\n",
        "        fd.seek(0)  # Set the buffer position to the beginning\n",
        "        return fd  # Return the buffer\n",
        "    else:\n",
        "        # If the input is a local file path, open the file in binary read mode\n",
        "        return open(url_or_path, 'rb')\n",
        "\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    if prompt.startswith('http://') or prompt.startswith('https://'):\n",
        "        # If the prompt starts with 'http://' or 'https://', split it by ':'\n",
        "        vals = prompt.rsplit(':', 2)\n",
        "        # Ensure there are at least two elements in vals, append empty strings if not\n",
        "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
        "    else:\n",
        "        # If the prompt does not start with 'http://' or 'https://', split it by ':'\n",
        "        vals = prompt.rsplit(':', 1)\n",
        "        # Ensure there are at least two elements in vals, append '1' if not\n",
        "        vals = vals + ['', '1'][len(vals):]\n",
        "    return vals[0], float(vals[1])\n",
        "\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size  # Size of the output cutouts\n",
        "        self.cutn = cutn  # Number of cutouts to generate\n",
        "        self.cut_pow = cut_pow  # Scaling factor for controlling the size of the cutouts\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]  # Get the height and width of the input tensor\n",
        "        max_size = min(sideX, sideY)  # Calculate the maximum size for cutouts\n",
        "        min_size = min(sideX, sideY, self.cut_size)  # Calculate the minimum size for cutouts\n",
        "        cutouts = []  # List to store generated cutouts\n",
        "        for _ in range(self.cutn):\n",
        "            # Randomly select a size for the cutout based on cut_pow\n",
        "            size = int(torch.rand([]) ** self.cut_pow * (max_size - min_size) + min_size)\n",
        "            # Randomly select offsets for the cutout\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            # Extract the cutout from the input tensor\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            # Resize the cutout to cut_size using adaptive average pooling\n",
        "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "        return torch.cat(cutouts)\n",
        "\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    # Normalize x and y along the last dimension\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    # Compute the spherical distance loss\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "\n",
        "def tv_loss(input):\n",
        "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
        "    # Pad the input tensor\n",
        "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "    # Compute differences along x and y axes\n",
        "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "    # Compute the mean of squared differences\n",
        "    return (x_diff ** 2 + y_diff ** 2).mean([1, 2, 3])\n",
        "\n",
        "\n",
        "def range_loss(input):\n",
        "    # Compute the mean squared error between input and its clamped version\n",
        "    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])\n"
      ],
      "metadata": {
        "id": "7Tok2FFRkkDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpbody2NCR7w"
      },
      "source": [
        "# Model settings\n",
        "\n",
        "model_config = model_and_diffusion_defaults()\n",
        "model_config.update({\n",
        "    'attention_resolutions': '32, 16, 8',\n",
        "    'class_cond': False,\n",
        "    'diffusion_steps': 1000,\n",
        "    'rescale_timesteps': True,\n",
        "    'timestep_respacing': '1000',  # Modify this value to decrease the number of\n",
        "                                   # timesteps.\n",
        "    'image_size': 256,\n",
        "    'learn_sigma': True,\n",
        "    'noise_schedule': 'linear',\n",
        "    'num_channels': 256,\n",
        "    'num_head_channels': 64,\n",
        "    'num_res_blocks': 2,\n",
        "    'resblock_updown': True,\n",
        "    'use_checkpoint': False,\n",
        "    'use_fp16': True,\n",
        "    'use_scale_shift_norm': True,\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnQjGugaDZPJ"
      },
      "source": [
        "# Load models\n",
        "# sets up the required models for image generation, including the diffusion model, CLIP model, and LPIPS model, and ensures they are properly loaded and prepared for inference on the chosen device.\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "model, diffusion = create_model_and_diffusion(**model_config) # model represents the generative model, and diffusion represents the diffusion process used in the generation.\n",
        "model.load_state_dict(torch.load('256x256_diffusion_uncond.pt', map_location='cpu')) # The pre-trained weights for the diffusion model are loaded from the file '256x256_diffusion_uncond.pt'.\n",
        "model.requires_grad_(False).eval().to(device)\n",
        "if model_config['use_fp16']:\n",
        "    model.convert_to_fp16()\n",
        "\n",
        "# The CLIP (Contrastive Language-Image Pre-training) model is loaded. In this case, it's the Vision Transformer (ViT) with the 'B' variant and an input resolution of 16 tokens.\n",
        "# The model is set to evaluation mode, and its gradients are turned off.\n",
        "clip_model = clip.load('ViT-B/16', jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "clip_size = clip_model.visual.input_resolution\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                 std=[0.26862954, 0.26130258, 0.27577711])\n",
        "lpips_model = lpips.LPIPS(net='vgg').to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zY-8I90LkC6"
      },
      "source": [
        "## Settings for this run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0PwzFZbLfcy"
      },
      "source": [
        "prompts = ['alien running on water']\n",
        "image_prompts = []\n",
        "batch_size = 1\n",
        "clip_guidance_scale = 1000  # Controls how much the image should look like the prompt.\n",
        "tv_scale = 150              # Controls the smoothness of the final output.\n",
        "range_scale = 50            # Controls how far out of range RGB values are allowed to be.\n",
        "cutn = 16\n",
        "n_batches = 1\n",
        "init_image = None   # This can be an URL or Colab local path and must be in quotes.\n",
        "skip_timesteps = 0  # This needs to be between approx. 200 and 500 when using an init image.\n",
        "                    # Higher values make the output look more like the init.\n",
        "init_scale = 0      # This enhances the effect of the init image, a good value is 1000.\n",
        "seed = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf9hTc8YLoLx"
      },
      "source": [
        "### Actually do the run..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5gODNAMEUCR"
      },
      "source": [
        "def do_run():\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    make_cutouts = MakeCutouts(clip_size, cutn)\n",
        "    side_x = side_y = model_config['image_size']\n",
        "\n",
        "    target_embeds, weights = [], []\n",
        "\n",
        "    for prompt in prompts:\n",
        "        txt, weight = parse_prompt(prompt)\n",
        "        target_embeds.append(clip_model.encode_text(clip.tokenize(txt).to(device)).float())\n",
        "        weights.append(weight)\n",
        "\n",
        "    for prompt in image_prompts:\n",
        "        path, weight = parse_prompt(prompt)\n",
        "        img = Image.open(fetch(path)).convert('RGB')\n",
        "        img = TF.resize(img, min(side_x, side_y, *img.size), transforms.InterpolationMode.LANCZOS)\n",
        "        batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "        embed = clip_model.encode_image(normalize(batch)).float()\n",
        "        target_embeds.append(embed)\n",
        "        weights.extend([weight / cutn] * cutn)\n",
        "\n",
        "    target_embeds = torch.cat(target_embeds)\n",
        "    weights = torch.tensor(weights, device=device)\n",
        "    if weights.sum().abs() < 1e-3:\n",
        "        raise RuntimeError('The weights must not sum to 0.')\n",
        "    weights /= weights.sum().abs()\n",
        "\n",
        "    init = None\n",
        "    if init_image is not None:\n",
        "        init = Image.open(fetch(init_image)).convert('RGB')\n",
        "        init = init.resize((side_x, side_y), Image.LANCZOS)\n",
        "        init = TF.to_tensor(init).to(device).unsqueeze(0).mul(2).sub(1)\n",
        "\n",
        "    cur_t = None\n",
        "\n",
        "    def cond_fn(x, t, out, y=None):\n",
        "        n = x.shape[0]\n",
        "        fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
        "        x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
        "        clip_in = normalize(make_cutouts(x_in.add(1).div(2)))\n",
        "        image_embeds = clip_model.encode_image(clip_in).float()\n",
        "        dists = spherical_dist_loss(image_embeds.unsqueeze(1), target_embeds.unsqueeze(0))\n",
        "        dists = dists.view([cutn, n, -1])\n",
        "        losses = dists.mul(weights).sum(2).mean(0)\n",
        "        tv_losses = tv_loss(x_in)\n",
        "        range_losses = range_loss(out['pred_xstart'])\n",
        "        loss = losses.sum() * clip_guidance_scale + tv_losses.sum() * tv_scale + range_losses.sum() * range_scale\n",
        "        if init is not None and init_scale:\n",
        "            init_losses = lpips_model(x_in, init)\n",
        "            loss = loss + init_losses.sum() * init_scale\n",
        "        return -torch.autograd.grad(loss, x)[0]\n",
        "\n",
        "    if model_config['timestep_respacing'].startswith('ddim'):\n",
        "        sample_fn = diffusion.ddim_sample_loop_progressive\n",
        "    else:\n",
        "        sample_fn = diffusion.p_sample_loop_progressive\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        cur_t = diffusion.num_timesteps - skip_timesteps - 1\n",
        "\n",
        "        samples = sample_fn(\n",
        "            model,\n",
        "            (batch_size, 3, side_y, side_x),\n",
        "            clip_denoised=False,\n",
        "            model_kwargs={},\n",
        "            cond_fn=cond_fn,\n",
        "            progress=True,\n",
        "            skip_timesteps=skip_timesteps,\n",
        "            init_image=init,\n",
        "            randomize_class=True,\n",
        "            cond_fn_with_grad=True,\n",
        "        )\n",
        "\n",
        "        for j, sample in enumerate(samples):\n",
        "            cur_t -= 1\n",
        "            if j % 100 == 0 or cur_t == -1:\n",
        "                print()\n",
        "                for k, image in enumerate(sample['pred_xstart']):\n",
        "                    filename = f'progress_{i * batch_size + k:05}.png'\n",
        "                    TF.to_pil_image(image.add(1).div(2).clamp(0, 1)).save(filename)\n",
        "                    tqdm.write(f'Batch {i}, step {j}, output {k}:')\n",
        "                    display.display(display.Image(filename))\n",
        "\n",
        "gc.collect()\n",
        "do_run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = ['sun rising behind the mountains']\n",
        "image_prompts = []\n",
        "batch_size = 1\n",
        "clip_guidance_scale = 1000  # Controls how much the image should look like the prompt.\n",
        "tv_scale = 150              # Controls the smoothness of the final output.\n",
        "range_scale = 50            # Controls how far out of range RGB values are allowed to be.\n",
        "cutn = 16\n",
        "n_batches = 1\n",
        "init_image = None   # This can be an URL or Colab local path and must be in quotes.\n",
        "skip_timesteps = 0  # This needs to be between approx. 200 and 500 when using an init image.\n",
        "                    # Higher values make the output look more like the init.\n",
        "init_scale = 0      # This enhances the effect of the init image, a good value is 1000.\n",
        "seed = 0"
      ],
      "metadata": {
        "id": "4CpDpf_Fl0Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def do_run():\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    make_cutouts = MakeCutouts(clip_size, cutn)\n",
        "    side_x = side_y = model_config['image_size']\n",
        "\n",
        "    target_embeds, weights = [], []\n",
        "\n",
        "    for prompt in prompts:\n",
        "        txt, weight = parse_prompt(prompt)\n",
        "        target_embeds.append(clip_model.encode_text(clip.tokenize(txt).to(device)).float())\n",
        "        weights.append(weight)\n",
        "\n",
        "    for prompt in image_prompts:\n",
        "        path, weight = parse_prompt(prompt)\n",
        "        img = Image.open(fetch(path)).convert('RGB')\n",
        "        img = TF.resize(img, min(side_x, side_y, *img.size), transforms.InterpolationMode.LANCZOS)\n",
        "        batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "        embed = clip_model.encode_image(normalize(batch)).float()\n",
        "        target_embeds.append(embed)\n",
        "        weights.extend([weight / cutn] * cutn)\n",
        "\n",
        "    target_embeds = torch.cat(target_embeds)\n",
        "    weights = torch.tensor(weights, device=device)\n",
        "    if weights.sum().abs() < 1e-3:\n",
        "        raise RuntimeError('The weights must not sum to 0.')\n",
        "    weights /= weights.sum().abs()\n",
        "\n",
        "    init = None\n",
        "    if init_image is not None:\n",
        "        init = Image.open(fetch(init_image)).convert('RGB')\n",
        "        init = init.resize((side_x, side_y), Image.LANCZOS)\n",
        "        init = TF.to_tensor(init).to(device).unsqueeze(0).mul(2).sub(1)\n",
        "\n",
        "    cur_t = None\n",
        "\n",
        "    def cond_fn(x, t, out, y=None):\n",
        "        n = x.shape[0]\n",
        "        fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
        "        x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
        "        clip_in = normalize(make_cutouts(x_in.add(1).div(2)))\n",
        "        image_embeds = clip_model.encode_image(clip_in).float()\n",
        "        dists = spherical_dist_loss(image_embeds.unsqueeze(1), target_embeds.unsqueeze(0))\n",
        "        dists = dists.view([cutn, n, -1])\n",
        "        losses = dists.mul(weights).sum(2).mean(0)\n",
        "        tv_losses = tv_loss(x_in)\n",
        "        range_losses = range_loss(out['pred_xstart'])\n",
        "        loss = losses.sum() * clip_guidance_scale + tv_losses.sum() * tv_scale + range_losses.sum() * range_scale\n",
        "        if init is not None and init_scale:\n",
        "            init_losses = lpips_model(x_in, init)\n",
        "            loss = loss + init_losses.sum() * init_scale\n",
        "        return -torch.autograd.grad(loss, x)[0]\n",
        "\n",
        "    if model_config['timestep_respacing'].startswith('ddim'):\n",
        "        sample_fn = diffusion.ddim_sample_loop_progressive\n",
        "    else:\n",
        "        sample_fn = diffusion.p_sample_loop_progressive\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        cur_t = diffusion.num_timesteps - skip_timesteps - 1\n",
        "\n",
        "        samples = sample_fn(\n",
        "            model,\n",
        "            (batch_size, 3, side_y, side_x),\n",
        "            clip_denoised=False,\n",
        "            model_kwargs={},\n",
        "            cond_fn=cond_fn,\n",
        "            progress=True,\n",
        "            skip_timesteps=skip_timesteps,\n",
        "            init_image=init,\n",
        "            randomize_class=True,\n",
        "            cond_fn_with_grad=True,\n",
        "        )\n",
        "\n",
        "        for j, sample in enumerate(samples):\n",
        "            cur_t -= 1\n",
        "            if j % 100 == 0 or cur_t == -1:\n",
        "                print()\n",
        "                for k, image in enumerate(sample['pred_xstart']):\n",
        "                    filename = f'progress_{i * batch_size + k:05}.png'\n",
        "                    TF.to_pil_image(image.add(1).div(2).clamp(0, 1)).save(filename)\n",
        "                    tqdm.write(f'Batch {i}, step {j}, output {k}:')\n",
        "                    display.display(display.Image(filename))\n",
        "\n",
        "gc.collect()\n",
        "do_run()"
      ],
      "metadata": {
        "id": "2ZoVLxEVl9MU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}